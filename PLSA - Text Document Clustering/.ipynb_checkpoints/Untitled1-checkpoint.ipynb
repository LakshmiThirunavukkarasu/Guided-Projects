{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43f32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "\n",
    "# 0. Create dataset\n",
    "X,Y = make_blobs(cluster_std=1.5,random_state=20,n_samples=500,centers=3)\n",
    "\n",
    "# Stratch dataset to get ellipsoid data\n",
    "X = np.dot(X,np.random.RandomState(0).randn(2,2))\n",
    "\n",
    "\n",
    "class GMM:\n",
    "\n",
    "    def __init__(self,X,number_of_sources,iterations):\n",
    "        self.iterations = iterations\n",
    "        self.number_of_sources = number_of_sources\n",
    "        self.X = X\n",
    "        self.mu = None\n",
    "        self.pi = None\n",
    "        self.cov = None\n",
    "        self.XY = None\n",
    "        \n",
    "    \n",
    "\n",
    "    \"\"\"Define a function which runs for iterations, iterations\"\"\"\n",
    "    def run(self):\n",
    "        self.reg_cov = 1e-6*np.identity(len(self.X[0]))\n",
    "        x,y = np.meshgrid(np.sort(self.X[:,0]),np.sort(self.X[:,1]))\n",
    "        self.XY = np.array([x.flatten(),y.flatten()]).T\n",
    "           \n",
    "                    \n",
    "        \"\"\" 1. Set the initial mu, covariance and pi values\"\"\"\n",
    "        self.mu = np.random.randint(min(self.X[:,0]),max(self.X[:,0]),size=(self.number_of_sources,len(self.X[0]))) # This is a nxm matrix since we assume n sources (n Gaussians) where each has m dimensions\n",
    "        self.cov = np.zeros((self.number_of_sources,len(X[0]),len(X[0]))) # We need a nxmxm covariance matrix for each source since we have m features --> We create symmetric covariance matrices with ones on the digonal\n",
    "        for dim in range(len(self.cov)):\n",
    "            np.fill_diagonal(self.cov[dim],5)\n",
    "\n",
    "\n",
    "        self.pi = np.ones(self.number_of_sources)/self.number_of_sources # Are \"Fractions\"\n",
    "        log_likelihoods = [] # In this list we store the log likehoods per iteration and plot them in the end to check if\n",
    "                             # if we have converged\n",
    "            \n",
    "        \"\"\"Plot the initial state\"\"\"    \n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax0 = fig.add_subplot(111)\n",
    "        ax0.scatter(self.X[:,0],self.X[:,1])\n",
    "        ax0.set_title('Initial state')\n",
    "        for m,c in zip(self.mu,self.cov):\n",
    "            c += self.reg_cov\n",
    "            multi_normal = multivariate_normal(mean=m,cov=c)\n",
    "            ax0.contour(np.sort(self.X[:,0]),np.sort(self.X[:,1]),multi_normal.pdf(self.XY).reshape(len(self.X),len(self.X)),colors='black',alpha=0.3)\n",
    "            ax0.scatter(m[0],m[1],c='grey',zorder=10,s=100)\n",
    "        \n",
    "        for i in range(self.iterations):               \n",
    "\n",
    "            \"\"\"E Step\"\"\"\n",
    "            r_ic = np.zeros((len(self.X),len(self.cov)))\n",
    "\n",
    "            for m,co,p,r in zip(self.mu,self.cov,self.pi,range(len(r_ic[0]))):\n",
    "                co+=self.reg_cov\n",
    "                mn = multivariate_normal(mean=m,cov=co)\n",
    "                r_ic[:,r] = p*mn.pdf(self.X)/np.sum([pi_c*multivariate_normal(mean=mu_c,cov=cov_c).pdf(X) for pi_c,mu_c,cov_c in zip(self.pi,self.mu,self.cov+self.reg_cov)],axis=0)\n",
    "\n",
    "            \"\"\"\n",
    "            The above calculation of r_ic is not that obvious why I want to quickly derive what we have done above.\n",
    "            First of all the nominator:\n",
    "            We calculate for each source c which is defined by m,co and p for every instance x_i, the multivariate_normal.pdf() value.\n",
    "            For each loop this gives us a 100x1 matrix (This value divided by the denominator is then assigned to r_ic[:,r] which is in \n",
    "            the end a 100x3 matrix).\n",
    "            Second the denominator:\n",
    "            What we do here is, we calculate the multivariate_normal.pdf() for every instance x_i for every source c which is defined by\n",
    "            pi_c, mu_c, and cov_c and write this into a list. This gives us a 3x100 matrix where we have 100 entrances per source c.\n",
    "            Now the formula wants us to add up the pdf() values given by the 3 sources for each x_i. Hence we sum up this list over axis=0.\n",
    "            This gives us then a list with 100 entries.\n",
    "            What we have now is FOR EACH LOOP a list with 100 entries in the nominator and a list with 100 entries in the denominator\n",
    "            where each element is the pdf per class c for each instance x_i (nominator) respectively the summed pdf's of classes c for each \n",
    "            instance x_i. Consequently we can now divide the nominator by the denominator and have as result a list with 100 elements which we\n",
    "            can then assign to r_ic[:,r] --> One row r per source c. In the end after we have done this for all three sources (three loops)\n",
    "            and run from r==0 to r==2 we get a matrix with dimensionallity 100x3 which is exactly what we want.\n",
    "            If we check the entries of r_ic we see that there mostly one element which is much larger than the other two. This is because\n",
    "            every instance x_i is much closer to one of the three gaussians (that is, much more likely to come from this gaussian) than\n",
    "            it is to the other two. That is practically speaing, r_ic gives us the fraction of the probability that x_i belongs to class\n",
    "            c over the probability that x_i belonges to any of the classes c (Probability that x_i occurs given the 3 Gaussians).\n",
    "            \"\"\"\n",
    "\n",
    "            \"\"\"M Step\"\"\"\n",
    "\n",
    "            # Calculate the new mean vector and new covariance matrices, based on the probable membership of the single x_i to classes c --> r_ic\n",
    "            self.mu = []\n",
    "            self.cov = []\n",
    "            self.pi = []\n",
    "            log_likelihood = []\n",
    "\n",
    "            for c in range(len(r_ic[0])):\n",
    "                m_c = np.sum(r_ic[:,c],axis=0)\n",
    "                mu_c = (1/m_c)*np.sum(self.X*r_ic[:,c].reshape(len(self.X),1),axis=0)\n",
    "                self.mu.append(mu_c)\n",
    "\n",
    "                # Calculate the covariance matrix per source based on the new mean\n",
    "                self.cov.append(((1/m_c)*np.dot((np.array(r_ic[:,c]).reshape(len(self.X),1)*(self.X-mu_c)).T,(self.X-mu_c)))+self.reg_cov)\n",
    "                # Calculate pi_new which is the \"fraction of points\" respectively the fraction of the probability assigned to each source \n",
    "                self.pi.append(m_c/np.sum(r_ic)) # Here np.sum(r_ic) gives as result the number of instances. This is logical since we know \n",
    "                                                # that the columns of each row of r_ic adds up to 1. Since we add up all elements, we sum up all\n",
    "                                                # columns per row which gives 1 and then all rows which gives then the number of instances (rows) \n",
    "                                                # in X --> Since pi_new contains the fractions of datapoints, assigned to the sources c,\n",
    "                                                # The elements in pi_new must add up to 1\n",
    "\n",
    "            \n",
    "            \n",
    "            \"\"\"Log likelihood\"\"\"\n",
    "            log_likelihoods.append(np.log(np.sum([k*multivariate_normal(self.mu[i],self.cov[j]).pdf(X) for k,i,j in zip(self.pi,range(len(self.mu)),range(len(self.cov)))])))\n",
    "\n",
    "            \n",
    "\n",
    "            \"\"\"\n",
    "            This process of E step followed by a M step is now iterated a number of n times. In the second step for instance,\n",
    "            we use the calculated pi_new, mu_new and cov_new to calculate the new r_ic which are then used in the second M step\n",
    "            to calculat the mu_new2 and cov_new2 and so on....\n",
    "            \"\"\"\n",
    "\n",
    "        fig2 = plt.figure(figsize=(10,10))\n",
    "        ax1 = fig2.add_subplot(111) \n",
    "        ax1.set_title('Log-Likelihood')\n",
    "        ax1.plot(range(0,self.iterations,1),log_likelihoods)\n",
    "        #plt.show()\n",
    "    \n",
    "    \"\"\"Predict the membership of an unseen, new datapoint\"\"\"\n",
    "    def predict(self,Y):\n",
    "        # PLot the point onto the fittet gaussians\n",
    "        fig3 = plt.figure(figsize=(10,10))\n",
    "        ax2 = fig3.add_subplot(111)\n",
    "        ax2.scatter(self.X[:,0],self.X[:,1])\n",
    "        for m,c in zip(self.mu,self.cov):\n",
    "            multi_normal = multivariate_normal(mean=m,cov=c)\n",
    "            ax2.contour(np.sort(self.X[:,0]),np.sort(self.X[:,1]),multi_normal.pdf(self.XY).reshape(len(self.X),len(self.X)),colors='black',alpha=0.3)\n",
    "            ax2.scatter(m[0],m[1],c='grey',zorder=10,s=100)\n",
    "            ax2.set_title('Final state')\n",
    "            for y in Y:\n",
    "                ax2.scatter(y[0],y[1],c='orange',zorder=10,s=100)\n",
    "        prediction = []        \n",
    "        for m,c in zip(self.mu,self.cov):  \n",
    "            #print(c)\n",
    "            prediction.append(multivariate_normal(mean=m,cov=c).pdf(Y)/np.sum([multivariate_normal(mean=mean,cov=cov).pdf(Y) for mean,cov in zip(self.mu,self.cov)]))\n",
    "        #plt.show()\n",
    "        return prediction\n",
    "         \n",
    "    \n",
    "    \n",
    "GMM = GMM(X,3,50)     \n",
    "GMM.run()\n",
    "GMM.predict([[0.5,0.5]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
